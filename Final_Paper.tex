
% ===========================================================================
% Title:
% ---------------------------------------------------------------------------
% to create Type I fonts type "dvips -P cmz -t letter <filename>"
% ===========================================================================
\documentclass[11pt]{article}       %--- LATEX 2e base
\usepackage{latexsym}               %--- LATEX 2e base
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}

%---------------- Tikz Flow D -----------------------------------------------

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=5cm, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

%---------------- Wide format -----------------------------------------------
\textwidth=6in \textheight=9in \oddsidemargin=0.25in
\evensidemargin=0.25in \topmargin=-0.5in
%--------------- Def., Theorem, Proof, etc. ---------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}
\usepackage{natbib}
\newenvironment{proof}           {\noindent{\bf Proof.} }%
                                 {\null\hfill$\Box$\par\medskip}
%--------------- Algorithm --------------------------------------------------
\newtheorem{algX}{Algorithm}
\newenvironment{algorithm}       {\begin{algX}\begin{em}}%
                                 {\par\noindent --- End of Algorithm ---
                                 \end{em}\end{algX}}
\newcommand{\step}[2]            {\begin{list}{}
                                  {  \setlength{\topsep}{0cm}
                                     \setlength{\partopsep}{0cm}
                                     \setlength{\leftmargin}{0.8cm}
                                     \setlength{\labelwidth}{0.7cm}
                                     \setlength{\labelsep}{0.1cm}    }
                                  \item[#1]#2    \end{list}}
                                 % usage: \begin{algorithm} \label{xyz}
                                 %        ... \step{(1)}{...} ...
                                 %        \end{algorithm}
%--------------- Figures ----------------------------------------------------
\usepackage{graphicx}

\newcommand{\includeFig}[3]      {\begin{figure}[htb] \begin{center}
                                 \includegraphics
                                 [width=4in,keepaspectratio] %comment this line to disable scaling
                                 {#2}\caption{\label{#1}#3} \end{center} \end{figure}}
                                 % usage: \includeFig{label}{file}{caption}


% ===========================================================================
\begin{document}
% ===========================================================================

% ############################################################################
% Title
% ############################################################################

\title{Parallel Genetic Algorithms on the GPU}


% ############################################################################
% Author(s) (no blank lines !)
\author{
% ############################################################################
George Savin\\
School of Computer Science\\
Carleton University\\
Ottawa, Canada K1S 5B6\\
{\em georgesavin@cmail.carleton.ca}
% ############################################################################
} % end-authors
% ############################################################################

\maketitle

% ############################################################################
% Abstract
% ############################################################################
\begin{abstract}
Genetic Algorithms are very powerful metaheuristics that have been successfully applied in  various disparate fields. While there are many sequential parts to the algorithm, a fair amount can and has been parallelized in MIMD and SIMD fashion. In this paper, we show a SIMT GPU solution to the the knapsack combinatorial problem. We do everything, including initialization of data on the device rather than the host CPU and show speed improvements across the board.
\end{abstract}

% ############################################################################
\section{Introduction} \label{intro}
% ############################################################################
Evolutionary Algorithms mimic behaviors of living things to search for optimal solutions. Genetic Algorithms are a well known subset of EAs, and employ genetic operators such as crossover, mutation and selection to "evolve" new solutions in a solution space over many iterations (generations). Even with the main loop of GAs being domain independent, two main components end up being problem specific : encoding and evaluation. These components may dramatically increase execution time, especially when a lot of candidate solutions need to be evaluated every generation.

Thankfully, Genetic Algorithms are amenable to parallelization and indeed researchers have experimented with parallelizing everything from individual operators to the whole process. Different GA model schemes such as master-slave and island models have been parallelized, with more attention put on island models for their natural inclination towards distributed execution.


Empirical and experimental researchers have predominantly been the drivers of GAs, mainly as optimization tools, however designing parallel algorithms can differ quite fundamentally given the underlying architecture. Implementations on the GPU have gained in popularity as each solution can be acted on independently for parts of the algorithm. However, researchers \cite{Van_Luong2010-mw}, myself included, have in some fashion found the cost of learning these parallelization techinques in tandem with GPU technologies to be painful.

Nevertheless, many problems have been successfully solved with GAs such financial pattern discovery, MAX-3SAT, layout problems, and ML hyperparameter selection \cite{Shivram2019-il, Amin2022-xd} among many others.

To best explore this topic and our implementation with the reader, the paper starts with a quick subsection on particulars of CUDA GPU programming. Section 2 is a literature review of all the work done in this space up to now, while Section 3 is our problem statement as to why master-slave models need to be revisited in light of the more recent island model dominance. Section 4 contains our proposed solution for our CPU and GPU implementations. In Section 5 we describe our positive findings and how we improved upon our initial GPU implementation. Finally Section 6 contains our conclusions and future work.

\subsection{CUDA GPU Programming}
Programming on a GPU requires your code to be ran at times on a separate device. NVIDIA provides CUDA\footnote{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html}, a general purpose parallelization and programming model. We use the subsets on the C/C++ programming languge to run our GA on  the GPU.

These NVIDIA GPUs consist of multiple streaming multiprocessors (SMs). When we define a kernel (the name of a function the host runs on the GPU), our blocks of threads get scheduled to run on these streaming multiprocessors. Each SM then executes these threads, with the lowest level of parallelism found at the \textit{warp} level. Warps are groupings of 32 threads. Threads on the same block can synchronize and also access shared memory. Blocks however cannot synchronize, and do not have any shared memory access and must rely on slower global memory.

When it comes to memory access and best practices, we aim for warp level memory coalescing\footnote{https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html} of global memory, leading to less request transactions in tandem with memory access patterns such as interleaved addressing. We also need to avoid serialization of operations when attempting to retrieve data as the GPU distributes memory over banks. 

Figure \ref{fig:gpu-info} highlights both the grid/block/thread structure as well as the different levels of memory available to our GPU programs.

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=.8\linewidth]{Figures/grid_block_mem.png}  
  \caption{Memory Allocation Levels}
  \label{fig:mem-allocation}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=.8\linewidth]{Figures/grid_block_thread.png}  
  \caption{Thread, Block, Grid Distribution}
  \label{fig:mutation}
\end{subfigure}
\caption{CUDA GPU Layout of Threads and Memory}
\label{fig:cgpu-info}
\end{figure}

% ############################################################################
\section{Literature Review} \label{litrev}
% ############################################################################

Using GPUs to accelerate Genetic Algorithms started to take off when NVIDIA released CUDA SDK 2.0 in 2008, allowing for more general programming tasks to be parallelized.  

A first intuitive approach is to move an operator onto the GPU that can efficiently be ran in parallel, such as the fitness evaluation across a population \cite{Maitre2009-wd}. Taking this idea further, generation of chromosomes could also moved to the GPU \cite{Cavuoti2013-oy}. Unfortunately, this constant transfer between CPU and GPU at every generation slowed down the runtime, and depending on the population size, may not have even been worth it \cite{robilliard2008population}. 

To overcome this transfer slowdown, master-slave models of a binary and real coded GAs were completely ported to the GPU \cite{Debattisti2009-su, Arora2010-ds}. Each operator (tournament selection, two-point and single point crossover, bitwise XOR mutation) became separate CUDA kernels. Both the crossover and mutation kernels in these cases suffered from the possibility of having the same chromosome operated on many times, causing inefficient usage and propagation of sub-optimal solutions.

Because of the architectural and operational constraints of the GPU, such as limited shared memory and expensive global memory lookups, models that aimed to reduce global communication fared better. Inversely, these models seemed to have less overall accuracy \cite{Zheng2011-zr}, while other works found this difference non-existant \cite{jahne2016overview}. Further follow-ups to quality of solutions between models have not been explored.

Island model GAs divide populations in the hope that diversity is preserved during evolution. On a GPU, this roughly translates to thread blocks as islands, and single threads to individuals. Within a block, fast memory access and synchronisation is available. Migration between islands occurs asynchronously.
Early island model implementations \cite{Pospichal2010-lf, Van_Luong2010-mw} used global memory only for migration. They were applied to numerical and combinatorial optimisations problems with tremendous speed-ups recorded. These speed-ups unfortunately were due mainly to comparisons with poorly optimized sequential GAs \cite{Jaros2012-ni}. When compared against properly parallelized GAs on CPUs, the speed-up was more in line with expected theoretical analysis. A similar revision of speed-up was seen with newer master-slave implementations \cite{Sinha2015-dk}. Trade-offs between speed and solution quality were analyzed and associated to parameter tuning, specifically island, generation and chromosome counts \cite{Sun2019-fj}.

During this time, research looking at optimizing GA GPU representations as well as technique improvements to leverage more parallelization was taking off. Building on the island model implementations, simulated annealing was shown to provide faster convergence when replacing mutation \cite{li2017parallel}. Memory layouts were also explored, and chromosome based layouts proved to increase locality and make better usage of caches \cite{jahne2016overview}. Different encoding representations also increased convergence speed at no detriment to solution quality \cite{Pedemonte2011-zu}. 

Newer work incorporates some of these techniques, as well as developing new ones targeting Island models almost exclusively. Random seed improvement lead to a solution technique that only generates a single random seed, yet still benefits from the uniqueness and speed one would typically get by generating seeds every generation \cite{Sun2019-fj}. Another recent paper used the idea of synchronous migration intervals to improve solution quality by avoiding unintended migrations \cite{Janssen2022-kr}. This one in particular also used the idea of allocating multiple threads per individual \cite{shah2010gpu} combined with better data organization and found that their techniques provided up to 18x speedups and better solution quality compared to the original IMGAs on the same hardware.
Newer work tries to use warp granularity to represent each island and reduce thread divergence \cite{Amin2022-xd}. They also perform synchronous and asynchronous replacement, improving solution quality, dubbing it Two-Replacement Policy. Unfortunately, these interesting island model changes were not compared to any prior island model implementations.

% ############################################################################
\section{Problem Statement} \label{probstat}
% ############################################################################
Given the take-over of Island Models as the default model for genetic algorithms on the GPU, re-visiting the master-slave model last seen in \cite{Cavuoti2013-oy} is a worthwhile endeavor. It gives us the chance to contrast the differences of these approaches on new GPU architecture and to apply the CUDA design methodology ``Assess, Parallelize, Optimize, Deploy" (APOD) \cite{bradley_2012} while we improve our model. This re-exploration is the first we know of in almost a decade, and will show that the master-slave model is a worthy alternative, and arguably should be viewed as the default when first choosing to pursue a SIMT/SIMD  acceleration of a GA. 

We target the 01-binary knapsack problem as it has important applications in every day life, and proves to be a good benchmark.

% ############################################################################
\section{Proposed Solution} \label{propsol}
% ############################################################################
Our CPU solution stays mostly faithful to the canonical Genetic Algorithm \cite{holland1984genetic}, and serves as a launching point for the APOD parallelization towards our GPU version. Figure \ref{fig:GA} is a flowchart of our implemented algorithm. There are two GPU solutions as one represents a first parallelization attempt, and a second improved version, based on APOD changes leveraging NVIDIA GPU architecture. Differences between the two will be highlighted in each of the following operators.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/Genetic Algorithm (1).png}
    \caption{The Genetic Algorithm logical path for both the CPU and GPU version. The GPU version kernalizes these steps}
    \label{fig:GA}
\end{figure}
\subsection{Encoding Schema}
A benefit of the binary knapsack is that both the genotype and phenotype representations are the same : a string of 1's and 0's indicating the presence or absence of the respective item at that index. For our problem, each 1 or 0 is represented by an integer, for a total array of length number of items. Figure \ref{fig:encoding} shows how the chromosome encoding lines up with the profit values and weight values of each item.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/knapsack_representation.png}
    \caption{Binary Knapsack chromosome encoding}
    \label{fig:encoding}
\end{figure}

\subsection{Initialization}
There are multiple ways to initialize chromosomes, and when we have a lot of items, it might make more sense to use a greedy approach, or some a priori information about the values and weights. For our own knapsack problem however, on the CPU we decided to use a bernoulli distribution and a mersenne twister pseudo-random generator to determine if a gene was 0 or 1. We also ensured that the total weight was not greater than the knapsack capacity by setting the remaining genes to 0 if this capacity would be crossed by a chromosome.

The GPU solution had to be slightly modified as none of these were readily available, and we had to rely on the NVIDIA cuRAND\footnote[1]{https://docs.nvidia.com/cuda/curand/index.html} library which used the \textit{XORWOW} generator. We also needed to split this step into two separate kernels \textit{initKernel} which sets a cuRAND handle for each chromosome, and the actual initialize kernel, \textit{initializeChromosomes} which sequentially iterated over genes and used a modulo to set either 0 or 1 values.

\subsection{Evaluation}
The CPU solution iterated over each chromosome sequentially and stored each respective score in the associated struct. A score of 1 was given to any chromosome that went over knapsack capacity. A running total, average and best score was kept throughout the loop.
The GPU evaluation was done in parallel on each chromosome in the  \textit{evaluateChromosome} kernel, and the same penalty of lowering the total score to 1 was applied. Existing reduction algorithms on the GPU (in our case to sum the scores of chromosomes) only used arrays of floats or ints, and not over structs, so we needed an additional \textit{pullScores} kernel to store this information into an array of scores that would then be reduced into a total sum by the \textit{reduce} kernel. This \textit{reduce} kernel was taken from the CUDA sample examples, and returned the summed result to the host.

A second improved GPU version leveraged a custom reduction we wrote and did not need to pull scores down from the chromosome structs. It also avoided moving the total score back to the host. This was all done in a kernel called \textit{sumReducer}.

\subsection{Reproduction}
Reproduction is really an overarching function that relies on selection of parent chromosomes, actual cross-over/reproduction of these parents, and mutation of the resulting offspring. These steps need to be done sequentially, and this restriction applies to the GPU implementation as well. Afterwards, all the offspring are copied into the original chromosome array. The more optimized GPU implementation moves this copy into its own kernel that works at the gene level rather than the chromosome level to squeeze out more throughput.

\subsubsection{Selection}
Selecting parents were done by a roulette selection implementation that picked unique parents for crossover. All other papers reviewed overlooked this uniqueness in selection. Figure \ref{fig:roulette} visualizes this roulette concept. Spinning the wheel was simply the modulo between a random number and the total score sum. We spun the wheel twice for one parent, and the second time around, the wheel was spun with the first parent removed. On the GPU, this function lived on the device, and was called at the chromosome thread level. It could not be further parallelized due to the need to select a first parent.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/roulette_selection.png}
    \caption{Roulette Selection of parent chromosomes}
    \label{fig:roulette}
\end{figure}

\subsubsection{Crossover}
The creation of offspring via crossover can be done in many ways, but for this project, we chose single point crossover. Figure \ref{fig:crossover} visualizes the single point crossover, where offspring are the parent chromosomes with swapped bits past the cutoff point. The CPU implementation allocates memory for the offspring, selects two parents and performs crossover. The GPU implementation is also at the chromosome thread level, and each thread performs this crossover, however only one of the offspring is chosen in a probabilistic manner. This was a change to ensure the algorithm would run on the GPU properly.

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=.8\linewidth]{Figures/crossover.png}  
  \caption{Single-Point Crossover}
  \label{fig:crossover}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=.8\linewidth]{Figures/mutation.png}  
  \caption{Gene Mutation}
  \label{fig:mutation}
\end{subfigure}
\caption{Single point Crossover and Mutation operations}
\label{fig:cross-mutate}
\end{figure}

\subsubsection{Mutation}
Finally mutation is when a gene flips from 1 to 0, or 0 to 1. It usually has a very small probability of occurring, and in our case, we kept it at 0.001. The first GPU implementation does this in parallel at the chromosome level, whereas the second implementation does it at the thread gene level increasing throughput.

\subsection{Experiments}
Knapsack item lists of various sizes were used, along with varying population sizes to debug and test both CPU and GPU programs, however the final experiments were for a 50 item knapsack and a 1000 item knapsack. With these two sizes, population sizes ranging from 32 to 1024 were tested against the CPU, 1st GPU implementation and post APOD 2nd GPU implementation.
NIVIDIA Nsight systems\footnote{https://developer.nvidia.com/nsight-systems} was used to profile each GPU run.

% ############################################################################
\section{Experimental Evaluation} \label{expeval}
% ############################################################################

\subsection{Hardware}
The CPU execution was done on an Intel(R) Core(TM) i7-9700K CPU @ 3.60GHz, with 32GB of RAM. The GPU used was a 12GB RTX 3060 on a Carleton cloud VM.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/cpu_gpu_runtimes.png}
    \caption{Results of the GA for our implementations across the GPU and CPU}
    \label{fig:runtime}
\end{figure}

\subsection{Results}
In Figure \ref{fig:runtime}, we can see at first glance that the second optimized GPU implementation is faster than the first one, and in fact, as population increases, also has a smaller gradient than the original GPU implementation. These linear increases are in contrast to the exponential rise of the CPU version, and indeed, both GPU solutions perform better than the CPU one. Near 1000 population size, the 2nd GPU implementation was 2.8x faster than the 1st GPU one, and 5.2x that of the CPU.

Shared memory was only leveraged in the reductions, and we had to rely on global memory to avoid serialization and some cache misses. We also had to rely on global memory due to the size of our arrays and the limited space on the blocks and threads. Table \ref{tab:nsys} goes over the APOD changes that ended up working in our favor while improving the first GPU algorithm. The Reproduction kernel took up 91\% of the all kernel execution time, so our optimizations aimed at reducing this number as well as the total time. Table \ref{tab:second_nsys} shows the halving effect of pulling out mutation from a device function to its own global kernel where each threads main job was to determine if a bit needed to be flipped. This concept was also extended as seen in Table \ref{tab:third_nsys} to the copying of offspring back into the parent chromosomes for the next generation, and this also led  to another halving of the total reproduction function runtime. Taking these out did produce some additional kernel call overhead, but it was well worth it in our case.

\begin{table}[]
\begin{subtable}[t]{\textwidth}
\begin{tabular}{@{}lllll@{}}
\toprule
Time (\%) & Total Time (ns) & Instances & Avg (ns)  & Name of Kernel          \\ \midrule
91.0      & 12,396,614,221     & 5,994     & 2,068,170.5 & GPUreproduceChromosomes \\
8.7       & 1,183,193,740      & 6,000     & 197,199.0  & evaluateChromosomes     \\
0.1       & 16,970,440      & 6,000     & 2,828.4  & sumReducer              \\
0.1       & 14,235,215      & 6     & 2,372,535.8   & initializeChromosomes      \\
0.1       & 12,529,457       & 6,000        & 2,088.2 & pullScores   \\
0.0      & 1,758,771       & 6         & 293,128.5 & initKernel              \\ \bottomrule
\end{tabular}
\caption{First APOD pass, using the custom sumReducer kernel. This reduced memory slowdown moving from device to host back to device}
\label{tab:first_nsys}
\end{subtable}
\newline
\vspace*{1 cm}
\newline
\begin{subtable}[t]{\textwidth}
\begin{tabular}{@{}lllll@{}}
\toprule
Time (\%) & Total Time (ns) & Instances & Avg (ns)  & Name of Kernel          \\ \midrule
83.5      & 6,782,679,608     & 5,994     & 1,131,578.2  & GPUreproduceChromosomes \\
14.9       & 1,209,217,233       & 6,000     & 201,536.2  & evaluateChromosomes     \\
1.0       & 85,121,386       & 5,994     & 14,201.1   & mutateChromosomes     \\
0.2       & 16,672,049      & 6,000     & 2,778.7  & sumReducer              \\
0.2       & 13,412,805      & 6     & 2,235,467.5   & initializeChromosomes                  \\
0.2       & 12,219,734       & 6,000        & 2,036.6 & pullScores   \\
0.0       & 1,705,907       & 6         & 284,317.8 & initKernel              \\ \bottomrule
\end{tabular}
\caption{We pull out the mutateChromosome function into its own kernel that now acts on the gene level. This added throughput halves the runtime of the reproduce kernel while our new kernel's runtime is negligible}
\label{tab:second_nsys}
\end{subtable}
\newline
\vspace*{1 cm}
\newline
\begin{subtable}[t]{\textwidth}
\begin{tabular}{@{}lllll@{}}
\toprule
Time (\%) & Total Time (ns) & Instances & Avg (ns)  & Name of Kernel          \\ \midrule
70.0      & 3,313,793,979     & 5,994     & 552,851.8  & GPUreproduceChromosomes \\
25.5       & 1,209,338,925       & 6,000     & 201,556.5  & evaluateChromosomes     \\
1.8       & 85,082,093       & 5,994     & 14,194.5   & mutateChromosomes     \\
1.7       & 82,532,905       & 5,994     & 13,769.3   & copyOffspringIntoChromosomes     \\
0.4       & 17,049,045     & 6,000     & 2,841.5  & sumReducer              \\
0.3       & 14,001,103      & 6     & 2,333,517.2   & initializeChromosomes                  \\
0.3       & 12,340,733       & 6,000        & 2,056.8  & pullScores   \\
0.0       & 1,717,651       & 6         & 286,275.2 & initKernel              \\ \bottomrule
\end{tabular}
\caption{The copying of offspring back into parent chromosomes can also be done at the gene level for maximal throughput. We can see doing this again halves the runtime of the Reproduce kernel, with negligible runtime for our new copyOffspring kernel}
\label{tab:third_nsys}
\end{subtable}
\caption{APOD Improvement via NVIDIA Nsight profiling the 1000 item knapsack runs across 6 (32,64,128,256,512,1024) population sizes}
\label{tab:nsys}
\end{table}

Naively, we assumed that it would then be possible to run all kernels and threads at the gene level, but we quickly realized there was a lot more thread divergence and less memory coalescence. In addition, complications occur when chromosomes are spread across two separate CUDA blocks. We can no longer do reductions across these blocks without using atomics, and we also lose the ability to sync between threads in case we want to continue with the sequential nature of our algorithm. We attempted to work around this by using atomicity specifically for the threads of a chromosome that spanned two blocks, but this caused too much divergence and the runtime was not any better than what we had achieved in the 2nd GPU implementation.

We tried various other memory representations in 2D to try and get around this limitation, but none of the runtimes improved in any significant way.

% ############################################################################
\section{Conclusions} \label{conc}
% ############################################################################

Not only is acceleration of Genetic Algorithms via GPUs faster than CPU implementations, a master-slave model is not too much technical overhead and will provide good speed-ups even in less than ideal implementations.

The remainder of the time is then spent on tweaking the memory layout and actions done in threads to increase cache hits and reducing thread/warp divergence as much as possible. This not only takes away from the actual problem being solved, but is highly tied to the actual GPU hardware available. NVIDIA has shown no qualms with changing architecture, and very recently announced a new paradigm of shared memory across blocks, which is something that we were not able to leverage in our project. Indeed, a GPU solution on a non NVIDIA GPU may look very different, and due to the painful learning process mentioned in the introduction, not many have ventured into this even less documented space.

Sometimes, as we saw in this project, threads working on every gene at once is an obvious and ideal speed-up, but one that does not always work given the encoding and problem at hand. Island Models in contrast try to avoid this by having only as many chromosomes as they can fit in their definition of an island (usually a block \cite{Janssen2022-kr}, but people have tried this at the warp level as well \cite{Amin2022-xd}). Unfortunately, the smaller amount of chromosomes per island comes at a cost of overall solution quality.


\subsection{Contributions}
We showed that not only is GPU acceleration of GAs do-able, but the direct master-slave approach with some architectural provisions is less technical overhead than island models and provides tempting speed-ups. 

We are also the first to walk through systematic changes between gene and chromosome level thread execution in our implementations, and show profiling results to support our decisions.

\subsection{Future Ideas}
We could not find any implementations of the Island GPU papers, so attempting to implement them and compare with our results would be interesting. Given the small and decreasing amount of chromosomes per island as the encodings get larger, we forsee less successful runs. However, newer variations of Island Models with the upcoming block group shared memory and synchronization would be worthy of exploration as there would be a new division of an island possible on GPUs.

No fine-tuning of grid and block sizes were done, so an extension looking at these and other possible parameters on the GPU would be good to get further speed-ups.

We also did not have time to explore more dense encodings of our chromosomes, which could have been done at the bit level and stuffed into integers. This simple change would for sure speed up many kernels as masks could then be used.

Finally, more intricate Genetic Algorithm idea's could be implemented, and a focus more on quality of score rather than just speed would be the differentiating factor between algorithms.



% ############################################################################
% Bibliography
% ############################################################################
\bibliographystyle{plain}
\bibliography{my-bibliography}     %loads my-bibliography.bib

% ============================================================================
\end{document}
% ============================================================================
